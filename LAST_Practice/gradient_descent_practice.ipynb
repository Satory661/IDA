{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97b015bb",
   "metadata": {},
   "source": [
    "# Практическая работа: градиентный спуск\n",
    "\n",
    "## Основные цели\n",
    "\n",
    "- Реализовать несколько вариантов градиентного спуска\n",
    "- Изучить влияние гиперпараметров на сходимость\n",
    "- Сравнить методы оптимизации на примерах из реальных данных\n",
    "- Применить градиентный спуск к разным задачам машинного обучения\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5efa4d",
   "metadata": {},
   "source": [
    "## Установка зависимостей и импорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c9554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_regression, make_classification, load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, log_loss\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "sns.set_palette(\"husl\")\n",
    "print(\"Библиотеки успешно импортированы!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d1666",
   "metadata": {},
   "source": [
    "## Задание 1: Простейший градиентный спуск\n",
    "\n",
    "Напишите реализацию градиентного спуска для простой квадратичной функции."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b65d524",
   "metadata": {},
   "source": [
    "### TODO 1.1: Градиентный спуск для f(x) = (x-3)² + 5\n",
    "\n",
    "Реализуйте алгоритм поиска минимума для функции f(x) = (x-3)² + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c6010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(x):\n",
    "    \"\"\"Целевая функция f(x) = (x-3)² + 5\"\"\"\n",
    "    pass\n",
    "\n",
    "def gradient_function(x):\n",
    "    \"\"\"Градиент функции f(x) = (x-3)² + 5\"\"\"\n",
    "    pass\n",
    "\n",
    "def gradient_descent_simple(start_x, learning_rate, num_iterations):\n",
    "    \"\"\"\n",
    "    Простой градиентный спуск\n",
    "    \"\"\"\n",
    "    x = start_x\n",
    "    history = [x]\n",
    "    for i in range(num_iterations):\n",
    "        pass\n",
    "    return x, history\n",
    "\n",
    "start_point = 0.0\n",
    "learning_rate = 0.1\n",
    "iterations = 50\n",
    "\n",
    "final_x, x_history = gradient_descent_simple(start_point, learning_rate, iterations)\n",
    "\n",
    "print(f\"Начальная точка: {start_point}\")\n",
    "print(f\"Финальная точка: {final_x:.6f}\")\n",
    "print(f\"Ожидаемый минимум: x = 3\")\n",
    "print(f\"Значение функции в минимуме: {objective_function(final_x):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc6bdf7",
   "metadata": {},
   "source": [
    "## Задание 2: Линейная регрессия с градиентным спуском\n",
    "\n",
    "Реализуйте полную версию градиентного спуска для линейной регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62853f1c",
   "metadata": {},
   "source": [
    "### TODO 2.1: Batch Gradient Descent для линейной регрессии\n",
    "\n",
    "Создайте класс для линейной регрессии с использованием градиентного спуска."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec13597",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGD:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.cost_history = []\n",
    "        self.theta = None\n",
    "    \n",
    "    def _add_bias(self, X):\n",
    "        \"\"\"Добавляет колонку единиц для свободного члена\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _compute_cost(self, X, y, theta):\n",
    "        \"\"\"Вычисляет функцию потерь MSE\"\"\"\n",
    "        m = X.shape[0]\n",
    "        pass\n",
    "    \n",
    "    def _compute_gradients(self, X, y, theta):\n",
    "        \"\"\"Вычисляет градиенты\"\"\"\n",
    "        m = X.shape[0]\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Обучает модель\"\"\"\n",
    "        X_with_bias = self._add_bias(X)\n",
    "        m, n = X_with_bias.shape\n",
    "        self.theta = np.random.normal(0, 0.01, n)\n",
    "        prev_cost = float('inf')\n",
    "        for i in range(self.max_iterations):\n",
    "            pass\n",
    "        print(f\"Обучение завершено за {len(self.cost_history)} итераций\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Делает предсказания\"\"\"\n",
    "        X_with_bias = self._add_bias(X)\n",
    "        pass\n",
    "\n",
    "np.random.seed(42)\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "model = LinearRegressionGD(learning_rate=0.1, max_iterations=1000)\n",
    "\n",
    "sklearn_model = LinearRegression()\n",
    "sklearn_model.fit(X_scaled, y)\n",
    "\n",
    "print(\"Сравнение с sklearn будет доступно после реализации ваших методов\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbd8c03",
   "metadata": {},
   "source": [
    "### TODO 2.2: Визуализация процесса обучения\n",
    "\n",
    "Создайте визуализацию процесса обучения и сравните с sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62943390",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42072ba",
   "metadata": {},
   "source": [
    "## Задание 3: Сравнение типов градиентного спуска\n",
    "\n",
    "Реализуйте и сравните Batch, Stochastic и Mini-batch градиентный спуск."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6037c44c",
   "metadata": {},
   "source": [
    "### TODO 3.1: Stochastic Gradient Descent\n",
    "\n",
    "Реализуйте стохастический градиентный спуск."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578028cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionSGD:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.cost_history = []\n",
    "        self.theta = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Обучение с использованием SGD\"\"\"\n",
    "        m, n = X.shape\n",
    "        X_with_bias = np.column_stack([np.ones(m), X])\n",
    "        self.theta = np.random.normal(0, 0.01, n + 1)\n",
    "        for iteration in range(self.max_iterations):\n",
    "            pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Предсказания\"\"\"\n",
    "        m = X.shape[0]\n",
    "        X_with_bias = np.column_stack([np.ones(m), X])\n",
    "        return X_with_bias.dot(self.theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72192d70",
   "metadata": {},
   "source": [
    "### TODO 3.2: Mini-batch Gradient Descent\n",
    "\n",
    "Реализуйте мини-пакетный градиентный спуск."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c56a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionMiniBatch:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=100, batch_size=32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.cost_history = []\n",
    "        self.theta = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Обучение с использованием Mini-batch GD\"\"\"\n",
    "        m, n = X.shape\n",
    "        X_with_bias = np.column_stack([np.ones(m), X])\n",
    "        self.theta = np.random.normal(0, 0.01, n + 1)\n",
    "        for iteration in range(self.max_iterations):\n",
    "            pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Предсказания\"\"\"\n",
    "        m = X.shape[0]\n",
    "        X_with_bias = np.column_stack([np.ones(m), X])\n",
    "        return X_with_bias.dot(self.theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d66d70",
   "metadata": {},
   "source": [
    "### TODO 3.3: Сравнение всех методов\n",
    "\n",
    "Сравните производительность всех трех методов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad79c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X_large, y_large = make_regression(n_samples=1000, n_features=5, noise=0.1, random_state=42)\n",
    "X_large_scaled = StandardScaler().fit_transform(X_large)\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Сравнение методов градиентного спуска:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338288e4",
   "metadata": {},
   "source": [
    "## Задание 4: Исследование Learning Rate\n",
    "\n",
    "Исследуйте влияние различных значений learning rate на сходимость."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784eb834",
   "metadata": {},
   "source": [
    "### TODO 4.1: Тестирование различных Learning Rate\n",
    "\n",
    "Протестируйте разные значения learning rate и проанализируйте результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783813f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4.1: Исследование влияния learning rate\n",
    "\n",
    "def test_learning_rates(X, y, learning_rates, max_iterations=200):\n",
    "    \"\"\"\n",
    "    Тестирует различные learning rates\n",
    "    \n",
    "    Parameters:\n",
    "    X, y: данные\n",
    "    learning_rates: список значений для тестирования\n",
    "    max_iterations: максимальное количество итераций\n",
    "    \n",
    "    Returns:\n",
    "    results: словарь с результатами\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        # ВАШ КОД ЗДЕСЬ\n",
    "        # 1. Создайте модель с данным learning rate\n",
    "        # 2. Обучите модель\n",
    "        # 3. Сохраните результаты (сходимость, финальная ошибка, количество итераций)\n",
    "        pass\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Тестируемые learning rates\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0, 1.5]\n",
    "\n",
    "# Запуск тестирования\n",
    "# results = test_learning_rates(X_scaled, y, learning_rates)\n",
    "\n",
    "# Анализ результатов\n",
    "print(\"Результаты тестирования learning rate:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Learning Rate':<15} {'Сходимость':<12} {'Итерации':<10} {'Финальная MSE':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# ВАШ КОД ДЛЯ ВЫВОДА РЕЗУЛЬТАТОВ\n",
    "\n",
    "# Визуализация результатов\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# ВАШ КОД ДЛЯ СОЗДАНИЯ 6 СУБПЛОТОВ (по одному для каждого learning rate)\n",
    "# Показать график сходимости для каждого learning rate\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a188f02",
   "metadata": {},
   "source": [
    "### TODO 4.2: Поиск оптимального Learning Rate\n",
    "\n",
    "Реализуйте простой алгоритм поиска оптимального learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebdfe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4.2: Поиск оптимального learning rate\n",
    "\n",
    "def find_optimal_learning_rate(X, y, lr_min=0.001, lr_max=1.0, num_trials=20):\n",
    "    \"\"\"\n",
    "    Находит оптимальный learning rate методом случайного поиска\n",
    "    \n",
    "    Parameters:\n",
    "    X, y: данные\n",
    "    lr_min, lr_max: диапазон поиска\n",
    "    num_trials: количество попыток\n",
    "    \n",
    "    Returns:\n",
    "    best_lr: лучший learning rate\n",
    "    best_mse: лучшая MSE\n",
    "    \"\"\"\n",
    "    best_lr = None\n",
    "    best_mse = float('inf')\n",
    "    results = []\n",
    "    \n",
    "    for i in range(num_trials):\n",
    "        # ВАШ КОД ЗДЕСЬ\n",
    "        # 1. Сгенерируйте случайный learning rate в заданном диапазоне\n",
    "        # 2. Обучите модель\n",
    "        # 3. Вычислите MSE\n",
    "        # 4. Обновите лучший результат\n",
    "        pass\n",
    "    \n",
    "    return best_lr, best_mse, results\n",
    "\n",
    "# Поиск оптимального learning rate\n",
    "# best_lr, best_mse, lr_results = find_optimal_learning_rate(X_scaled, y)\n",
    "\n",
    "print(f\"Оптимальный learning rate: {best_lr:.6f}\")\n",
    "print(f\"Лучшая MSE: {best_mse:.6f}\")\n",
    "\n",
    "# Визуализация результатов поиска\n",
    "# ВАШ КОД ДЛЯ ВИЗУАЛИЗАЦИИ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffa1230",
   "metadata": {},
   "source": [
    "## Задание 5: Логистическая регрессия с градиентным спуском\n",
    "\n",
    "Примените градиентный спуск к задаче классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498a7757",
   "metadata": {},
   "source": [
    "### TODO 5.1: Реализация логистической регрессии\n",
    "\n",
    "Реализуйте логистическую регрессию с использованием градиентного спуска."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d97038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5.1: Логистическая регрессия с градиентным спуском\n",
    "\n",
    "class LogisticRegressionGD:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.cost_history = []\n",
    "        self.theta = None\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Сигмоидная функция\"\"\"\n",
    "        # ВАШ КОД ЗДЕСЬ\n",
    "        # Добавьте защиту от переполнения\n",
    "        pass\n",
    "    \n",
    "    def _compute_cost(self, X, y, theta):\n",
    "        \"\"\"Вычисляет логистическую функцию потерь\"\"\"\n",
    "        m = X.shape[0]\n",
    "        # ВАШ КОД ЗДЕСЬ\n",
    "        # Формула: J = -(1/m) * Σ[y*log(h) + (1-y)*log(1-h)]\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Обучение модели\"\"\"\n",
    "        m, n = X.shape\n",
    "        X_with_bias = np.column_stack([np.ones(m), X])\n",
    "        \n",
    "        # Инициализация параметров\n",
    "        self.theta = np.random.normal(0, 0.01, n + 1)\n",
    "        \n",
    "        prev_cost = float('inf')\n",
    "        \n",
    "        for i in range(self.max_iterations):\n",
    "            # ВАШ КОД ЗДЕСЬ\n",
    "            # 1. Вычислите предсказания через сигмоиду\n",
    "            # 2. Вычислите функцию потерь\n",
    "            # 3. Вычислите градиенты\n",
    "            # 4. Обновите параметры\n",
    "            # 5. Проверьте сходимость\n",
    "            pass\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Предсказание вероятностей\"\"\"\n",
    "        # ВАШ КОД ЗДЕСЬ\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Предсказание классов\"\"\"\n",
    "        # ВАШ КОД ЗДЕСЬ\n",
    "        pass\n",
    "\n",
    "# Создание данных для классификации\n",
    "X_class, y_class = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n",
    "                                      n_informative=2, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# Стандартизация\n",
    "scaler_class = StandardScaler()\n",
    "X_class_scaled = scaler_class.fit_transform(X_class)\n",
    "\n",
    "# Тестирование логистической регрессии\n",
    "# log_model = LogisticRegressionGD(learning_rate=0.1, max_iterations=1000)\n",
    "# log_model.fit(X_class_scaled, y_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4058ea91",
   "metadata": {},
   "source": [
    "### TODO 5.2: Сравнение с sklearn и визуализация\n",
    "\n",
    "Сравните вашу реализацию с sklearn и создайте визуализацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584aff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5.2: Сравнение и визуализация логистической регрессии\n",
    "\n",
    "# Сравнение с sklearn\n",
    "sklearn_log = LogisticRegression(max_iter=1000)\n",
    "sklearn_log.fit(X_class_scaled, y_class)\n",
    "\n",
    "# Предсказания\n",
    "# your_predictions = log_model.predict(X_class_scaled)\n",
    "sklearn_predictions = sklearn_log.predict(X_class_scaled)\n",
    "\n",
    "print(\"Сравнение логистической регрессии:\")\n",
    "# print(f\"Accuracy (ваша реализация): {accuracy_score(y_class, your_predictions):.4f}\")\n",
    "print(f\"Accuracy (sklearn): {accuracy_score(y_class, sklearn_predictions):.4f}\")\n",
    "\n",
    "# Визуализация\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# График 1: Данные и границы решения\n",
    "ax1.scatter(X_class_scaled[y_class==0, 0], X_class_scaled[y_class==0, 1], \n",
    "           c='red', marker='o', alpha=0.6, label='Класс 0')\n",
    "ax1.scatter(X_class_scaled[y_class==1, 0], X_class_scaled[y_class==1, 1], \n",
    "           c='blue', marker='s', alpha=0.6, label='Класс 1')\n",
    "\n",
    "# ВАШ КОД ДЛЯ РИСОВАНИЯ ГРАНИЦЫ РЕШЕНИЯ\n",
    "\n",
    "ax1.set_xlabel('Признак 1')\n",
    "ax1.set_ylabel('Признак 2')\n",
    "ax1.set_title('Логистическая регрессия')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# График 2: Сходимость функции потерь\n",
    "# ВАШ КОД ДЛЯ ГРАФИКА СХОДИМОСТИ\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3de250c",
   "metadata": {},
   "source": [
    "## Задание 6: Продвинутые методы оптимизации\n",
    "\n",
    "Реализуйте и сравните различные методы оптимизации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba2589c",
   "metadata": {},
   "source": [
    "### TODO 6.1: Gradient Descent с Momentum\n",
    "\n",
    "Реализуйте градиентный спуск с импульсом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7680271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 6.1: Реализация Momentum\n",
    "\n",
    "class LinearRegressionMomentum:\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9, max_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.max_iterations = max_iterations\n",
    "        self.cost_history = []\n",
    "        self.theta = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Обучение с использованием momentum\"\"\"\n",
    "        m, n = X.shape\n",
    "        X_with_bias = np.column_stack([np.ones(m), X])\n",
    "        \n",
    "        # Инициализация\n",
    "        self.theta = np.random.normal(0, 0.01, n + 1)\n",
    "        velocity = np.zeros_like(self.theta)\n",
    "        \n",
    "        for i in range(self.max_iterations):\n",
    "            # ВАШ КОД ЗДЕСЬ\n",
    "            # 1. Вычислите предсказания\n",
    "            # 2. Вычислите функцию потерь\n",
    "            # 3. Вычислите градиенты\n",
    "            # 4. Обновите velocity: v = β*v + (1-β)*grad\n",
    "            # 5. Обновите параметры: θ = θ - α*v\n",
    "            pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Предсказания\"\"\"\n",
    "        m = X.shape[0]\n",
    "        X_with_bias = np.column_stack([np.ones(m), X])\n",
    "        return X_with_bias.dot(self.theta)\n",
    "\n",
    "# Тестирование Momentum\n",
    "# momentum_model = LinearRegressionMomentum(learning_rate=0.01, momentum=0.9)\n",
    "# momentum_model.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be96f543",
   "metadata": {},
   "source": [
    "### TODO 6.2: Сравнение методов оптимизации\n",
    "\n",
    "Сравните обычный GD, GD с momentum и другие методы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbcb45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 6.2: Сравнение методов оптимизации\n",
    "\n",
    "# Создайте сложную функцию для демонстрации преимуществ momentum\n",
    "np.random.seed(42)\n",
    "X_complex = np.random.randn(300, 3)\n",
    "# Добавьте корреляцию между признаками\n",
    "X_complex[:, 1] = X_complex[:, 0] + 0.5 * np.random.randn(300)\n",
    "X_complex[:, 2] = X_complex[:, 0] - 0.3 * X_complex[:, 1] + 0.2 * np.random.randn(300)\n",
    "y_complex = 3 * X_complex[:, 0] + 2 * X_complex[:, 1] - X_complex[:, 2] + np.random.randn(300) * 0.1\n",
    "\n",
    "# Словарь для хранения моделей\n",
    "optimization_models = {}\n",
    "\n",
    "# Обычный GD\n",
    "# ВАШ КОД ЗДЕСЬ\n",
    "\n",
    "# GD с Momentum\n",
    "# ВАШ КОД ЗДЕСЬ\n",
    "\n",
    "# GD с высоким learning rate (для демонстрации проблем)\n",
    "# ВАШ КОД ЗДЕСЬ\n",
    "\n",
    "# Сравнение результатов\n",
    "print(\"Сравнение методов оптимизации:\")\n",
    "print(\"-\" * 60)\n",
    "# ВАШ КОД ДЛЯ ВЫВОДА РЕЗУЛЬТАТОВ\n",
    "\n",
    "# Визуализация\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# График 1: Сравнение сходимости\n",
    "plt.subplot(1, 3, 1)\n",
    "# ВАШ КОД ДЛЯ ГРАФИКА СХОДИМОСТИ\n",
    "\n",
    "# График 2: Первые 50 итераций\n",
    "plt.subplot(1, 3, 2)\n",
    "# ВАШ КОД ДЛЯ ДЕТАЛЬНОГО ВИДА\n",
    "\n",
    "# График 3: Финальные результаты\n",
    "plt.subplot(1, 3, 3)\n",
    "# ВАШ КОД ДЛЯ СРАВНЕНИЯ ФИНАЛЬНЫХ РЕЗУЛЬТАТОВ\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e330c2",
   "metadata": {},
   "source": [
    "## Задание 7: Полиномиальная регрессия\n",
    "\n",
    "Примените градиентный спуск к полиномиальной регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db93a16",
   "metadata": {},
   "source": [
    "### TODO 7.1: Полиномиальные признаки\n",
    "\n",
    "Создайте полиномиальные признаки и примените градиентный спуск."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6c7ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 7.1: Полиномиальная регрессия с градиентным спуском\n",
    "\n",
    "def create_polynomial_features(X, degree):\n",
    "    \"\"\"\n",
    "    Создает полиномиальные признаки до заданной степени\n",
    "    \n",
    "    Parameters:\n",
    "    X: исходные признаки (n_samples, n_features)\n",
    "    degree: максимальная степень полинома\n",
    "    \n",
    "    Returns:\n",
    "    X_poly: полиномиальные признаки\n",
    "    \"\"\"\n",
    "    # ВАШ КОД ЗДЕСЬ\n",
    "    # Подсказка: используйте nested loops для создания комбинаций признаков\n",
    "    pass\n",
    "\n",
    "# Создание нелинейных данных\n",
    "np.random.seed(42)\n",
    "X_nonlinear = np.random.uniform(-2, 2, (100, 1))\n",
    "y_nonlinear = 0.5 * X_nonlinear.flatten()**3 - 2 * X_nonlinear.flatten()**2 + X_nonlinear.flatten() + np.random.normal(0, 0.5, 100)\n",
    "\n",
    "# Создание полиномиальных признаков\n",
    "# X_poly = create_polynomial_features(X_nonlinear, degree=3)\n",
    "\n",
    "# Стандартизация\n",
    "# scaler_poly = StandardScaler()\n",
    "# X_poly_scaled = scaler_poly.fit_transform(X_poly)\n",
    "\n",
    "# Обучение полиномиальной регрессии\n",
    "# poly_model = LinearRegressionGD(learning_rate=0.01, max_iterations=1000)\n",
    "# poly_model.fit(X_poly_scaled, y_nonlinear)\n",
    "\n",
    "# Визуализация результатов\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# График 1: Исходные данные\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X_nonlinear, y_nonlinear, alpha=0.6)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Нелинейные данные')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# График 2: Полиномиальная регрессия\n",
    "plt.subplot(1, 3, 2)\n",
    "# ВАШ КОД ДЛЯ ВИЗУАЛИЗАЦИИ ПОЛИНОМИАЛЬНОЙ РЕГРЕССИИ\n",
    "\n",
    "# График 3: Сходимость\n",
    "plt.subplot(1, 3, 3)\n",
    "# ВАШ КОД ДЛЯ ГРАФИКА СХОДИМОСТИ\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0c2ea2",
   "metadata": {},
   "source": [
    "## Задание 8: Анализ и выводы\n",
    "\n",
    "Проанализируйте все полученные результаты и сделайте выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b57b2f",
   "metadata": {},
   "source": [
    "### TODO 8.1: Создание итогового отчета\n",
    "\n",
    "Создайте сводный анализ всех экспериментов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedddaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 8.1: Итоговый анализ результатов\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ИТОГОВЫЙ ОТЧЕТ ПО ГРАДИЕНТНОМУ СПУСКУ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Сравнение типов градиентного спуска\n",
    "print(\"\\n1. СРАВНЕНИЕ ТИПОВ ГРАДИЕНТНОГО СПУСКА:\")\n",
    "print(\"-\" * 50)\n",
    "# ВАШ КОД ЗДЕСЬ - сравните Batch, SGD, Mini-batch\n",
    "\n",
    "# 2. Влияние Learning Rate\n",
    "print(\"\\n2. ВЛИЯНИЕ LEARNING RATE:\")\n",
    "print(\"-\" * 30)\n",
    "# ВАШ КОД ЗДЕСЬ - проанализируйте результаты разных learning rates\n",
    "\n",
    "# 3. Эффективность методов оптимизации\n",
    "print(\"\\n3. МЕТОДЫ ОПТИМИЗАЦИИ:\")\n",
    "print(\"-\" * 25)\n",
    "# ВАШ КОД ЗДЕСЬ - сравните обычный GD и Momentum\n",
    "\n",
    "# 4. Применение к разным задачам\n",
    "print(\"\\n4. ПРИМЕНЕНИЕ К РАЗНЫМ ЗАДАЧАМ:\")\n",
    "print(\"-\" * 35)\n",
    "# ВАШ КОД ЗДЕСЬ - сравните линейную, логистическую и полиномиальную регрессию\n",
    "\n",
    "# 5. Рекомендации\n",
    "print(\"\\n5. ПРАКТИЧЕСКИЕ РЕКОМЕНДАЦИИ:\")\n",
    "print(\"-\" * 35)\n",
    "recommendations = [\n",
    "    \"Для малых данных используйте...\",\n",
    "    \"Для больших данных предпочтительнее...\",\n",
    "    \"При нестабильной сходимости попробуйте...\",\n",
    "    \"Для быстрого прототипирования начните с...\",\n",
    "    \"Learning rate стоит выбирать в диапазоне...\"\n",
    "]\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5910d77b",
   "metadata": {},
   "source": [
    "### TODO 8.2: Финальная визуализация\n",
    "\n",
    "Создайте финальную визуализацию, объединяющую все результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a2643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 8.2: Финальная сводная визуализация\n",
    "\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# График 1: Сравнение типов GD\n",
    "plt.subplot(3, 3, 1)\n",
    "# ВАШ КОД ДЛЯ СРАВНЕНИЯ ТИПОВ GD\n",
    "\n",
    "# График 2: Влияние Learning Rate\n",
    "plt.subplot(3, 3, 2)\n",
    "# ВАШ КОД ДЛЯ ВИЗУАЛИЗАЦИИ LEARNING RATES\n",
    "\n",
    "# График 3: Методы оптимизации\n",
    "plt.subplot(3, 3, 3)\n",
    "# ВАШ КОД ДЛЯ СРАВНЕНИЯ МЕТОДОВ ОПТИМИЗАЦИИ\n",
    "\n",
    "# График 4: Линейная регрессия\n",
    "plt.subplot(3, 3, 4)\n",
    "# ВАШ КОД ДЛЯ ЛИНЕЙНОЙ РЕГРЕССИИ\n",
    "\n",
    "# График 5: Логистическая регрессия\n",
    "plt.subplot(3, 3, 5)\n",
    "# ВАШ КОД ДЛЯ ЛОГИСТИЧЕСКОЙ РЕГРЕССИИ\n",
    "\n",
    "# График 6: Полиномиальная регрессия\n",
    "plt.subplot(3, 3, 6)\n",
    "# ВАШ КОД ДЛЯ ПОЛИНОМИАЛЬНОЙ РЕГРЕССИИ\n",
    "\n",
    "# График 7: Сводка по точности\n",
    "plt.subplot(3, 3, 7)\n",
    "# ВАШ КОД ДЛЯ СВОДКИ ПО ТОЧНОСТИ\n",
    "\n",
    "# График 8: Сводка по времени сходимости\n",
    "plt.subplot(3, 3, 8)\n",
    "# ВАШ КОД ДЛЯ СВОДКИ ПО ВРЕМЕНИ\n",
    "\n",
    "# График 9: Общие рекомендации (текст)\n",
    "plt.subplot(3, 3, 9)\n",
    "plt.text(0.1, 0.9, \"ВЫВОДЫ:\", fontsize=14, fontweight='bold', transform=plt.gca().transAxes)\n",
    "conclusions = [\n",
    "    \"• Batch GD: точнее, но медленнее\",\n",
    "    \"• SGD: быстрее, но нестабильнее\", \n",
    "    \"• Mini-batch: лучший компромисс\",\n",
    "    \"• Learning rate критически важен\",\n",
    "    \"• Momentum улучшает сходимость\"\n",
    "]\n",
    "\n",
    "for i, conclusion in enumerate(conclusions):\n",
    "    plt.text(0.1, 0.7 - i*0.1, conclusion, fontsize=10, transform=plt.gca().transAxes)\n",
    "\n",
    "plt.gca().set_xticks([])\n",
    "plt.gca().set_yticks([])\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a36e76c",
   "metadata": {},
   "source": [
    "## Дополнительные задания (по желанию)\n",
    "\n",
    "### Задание 9: Реализация Adam оптимизатора\n",
    "\n",
    "Если у вас есть дополнительное время, реализуйте Adam оптимизатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b430b6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Бонусное задание: Adam оптимизатор\n",
    "\n",
    "class LinearRegressionAdam:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, max_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.max_iterations = max_iterations\n",
    "        self.cost_history = []\n",
    "        self.theta = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Обучение с использованием Adam\"\"\"\n",
    "        m, n = X.shape\n",
    "        X_with_bias = np.column_stack([np.ones(m), X])\n",
    "        \n",
    "        # Инициализация\n",
    "        self.theta = np.random.normal(0, 0.01, n + 1)\n",
    "        m_t = np.zeros_like(self.theta)  # Первый момент\n",
    "        v_t = np.zeros_like(self.theta)  # Второй момент\n",
    "        \n",
    "        for t in range(1, self.max_iterations + 1):\n",
    "            # ВАШ КОД ЗДЕСЬ (если решите реализовать)\n",
    "            # Adam алгоритм:\n",
    "            # 1. Вычислите градиенты\n",
    "            # 2. Обновите моменты: m_t = β1*m_t + (1-β1)*grad\n",
    "            # 3. Обновите моменты: v_t = β2*v_t + (1-β2)*grad²\n",
    "            # 4. Скорректируйте bias: m̂_t = m_t/(1-β1^t), v̂_t = v_t/(1-β2^t)\n",
    "            # 5. Обновите параметры: θ = θ - α*m̂_t/(√v̂_t + ε)\n",
    "            pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Предсказания\"\"\"\n",
    "        m = X.shape[0]\n",
    "        X_with_bias = np.column_stack([np.ones(m), X])\n",
    "        return X_with_bias.dot(self.theta)\n",
    "\n",
    "# Тестирование Adam (если реализован)\n",
    "# adam_model = LinearRegressionAdam()\n",
    "# adam_model.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e5e46f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Домашнее задание\n",
    "\n",
    "1. Завершите все TODO задания в этом ноутбуке\n",
    "2. Реализуйте градиентный спуск для задачи множественной классификации (3+ классов)\n",
    "3. Сравните производительность всех методов на реальном датасете из sklearn\n",
    "4. Создайте интерактивную визуализацию процесса градиентного спуска (используйте matplotlib animation)\n",
    "5. Напишите краткий отчет (1-2 страницы) с выводами и рекомендациями\n",
    "\n",
    "**Критерии оценки:**\n",
    "- Корректность реализации алгоритмов (40%)\n",
    "- Качество экспериментов и анализа (25%)\n",
    "- Визуализации и их интерпретация (20%)\n",
    "- Выводы и практические рекомендации (15%)\n",
    "\n",
    "**Срок сдачи:** следующее занятие  \n",
    "**Формат:** Jupyter notebook с выполненными заданиями + отчет"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
